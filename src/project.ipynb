{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#  Project - Natural Language Processing                 \n",
    "#   Author: MarÃ­a Sauras & Oscar Llorente                      \n",
    "#   Date: 08/01/2023                                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'preprocessing' from 'src.utils' (/Users/ledito/Desktop/euro_sci/src/utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcollections\u001b[39;00m \u001b[39mimport\u001b[39;00m Counter\n\u001b[1;32m     23\u001b[0m \u001b[39m# own modules\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m preprocessing, extract_data, BasicModel\n\u001b[1;32m     26\u001b[0m \u001b[39m# ignore warnings\u001b[39;00m\n\u001b[1;32m     27\u001b[0m warnings\u001b[39m.\u001b[39mfilterwarnings(\u001b[39m'\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'preprocessing' from 'src.utils' (/Users/ledito/Desktop/euro_sci/src/utils.py)"
     ]
    }
   ],
   "source": [
    "# machine learning and NLP libraries\n",
    "import torchtext\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import scipy.sparse as scsp\n",
    "import seaborn as sns\n",
    "from scipy.sparse import csr_matrix, hstack, triu\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score, classification_report\n",
    "from gensim.matutils import corpus2dense, corpus2csc\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel, LdaMulticore\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "# other libraries\n",
    "import warnings\n",
    "from collections import Counter\n",
    "\n",
    "# own modules\n",
    "from src.utils import preprocessing, extract_data, BasicModel\n",
    "\n",
    "# ignore warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix, triu"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Extraction and Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, labels, costs, contributions = extract_data('data/projects.xlsx', 'data/SciVocCodes.xlsx', 0)\n",
    "texts = preprocessing(texts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Vectorization Techniques"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-Idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get texts in right format\n",
    "D = Dictionary([sentence.split(' ') for sentence in texts])\n",
    "n_tokens = len(D)\n",
    "reviews_bow = [D.doc2bow(doc.split(' ')) for doc in texts]\n",
    "\n",
    "# compute tfidf representations\n",
    "tfidf = TfidfModel(reviews_bow)\n",
    "reviews_tfidf = tfidf[reviews_bow]\n",
    "\n",
    "# create sparse corpus\n",
    "num_docs = len(reviews_bow)\n",
    "corpus_tfidf_sparse = corpus2csc(reviews_tfidf, num_terms=n_tokens, num_docs=num_docs).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BasicModel(corpus_tfidf_sparse, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.6597171278603078\n"
     ]
    }
   ],
   "source": [
    "print(f'accuracy: {model.accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.82      0.70       488\n",
      "           1       0.79      0.60      0.68      1326\n",
      "           2       0.61      0.73      0.67       659\n",
      "           3       0.38      0.41      0.40        73\n",
      "           4       0.64      0.64      0.64       549\n",
      "           5       0.65      0.76      0.70       111\n",
      "\n",
      "    accuracy                           0.67      3206\n",
      "   macro avg       0.61      0.66      0.63      3206\n",
      "weighted avg       0.69      0.67      0.67      3206\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.classification_report)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since to make use of the LDA model it is necessary to define the number of topics we want in our model, we will make use of the perplexity metric.\n",
    "\n",
    "Perplexity is a metric used to evaluate the performance of a topic model in natural language processing. It is defined as the inverse of the logarithm of the average probability of the documents in the test set under the model. \n",
    "\n",
    "An LDA model with a lower perplexity is considered more accurate and has a better generalization ability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to determine the optimal number of topics in the LDA model --> We calculate it using the perplexity metric.\n",
    "def ntopics_optim(num_topics,reviews_bow,n_tokens):\n",
    "    ldag = LdaModel(corpus=reviews_bow, id2word=D, num_topics=num_topics)\n",
    "    return ldag.log_perplexity(reviews_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_perplexity(nt,perplexity):\n",
    "    plt.plot(nt, perplexity, marker='o', linestyle='--', color='r', label='Perplexity') \n",
    "    plt.title(\"Perplexity plot to determine the number of topics\")\n",
    "    plt.ylabel('perplexity value')\n",
    "    plt.xlabel('num of topics') \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nt = [100, 200, 500] \n",
    "perplexity = []\n",
    "for i in range(len(nt)):\n",
    "    p = ntopics_optim(nt[i],reviews_bow,n_tokens)\n",
    "    perplexity.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_perplexity(nt, perplexity)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute results with best LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_model(reviews_bow, num_topics):\n",
    "    # compute LDA model\n",
    "    ldag = LdaModel(corpus=reviews_bow, id2word=D, num_topics=num_topics)\n",
    "    # create sparse corpus\n",
    "    corpus_ldag = ldag[reviews_bow]\n",
    "    num_docs = len(reviews_bow)\n",
    "    return corpus2csc(corpus_ldag, num_terms=n_tokens, num_docs=num_docs).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_ldag_sparse = lda_model(reviews_bow, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BasicModel(corpus_ldag_sparse, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'accuracy: {model.accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.classification_report)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will compute Glove Embeddings of 300 hundred dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.zeros((len(texts), 300))\n",
    "glove = torchtext.vocab.GloVe(name='6B', dim=300)\n",
    "for i in range(len(texts)):\n",
    "    embeddings[i] = glove.get_vecs_by_tokens(texts[i].split(' '), lower_case_backup=True).mean(dim=0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BasicModel(embeddings, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.6768857690045743\n"
     ]
    }
   ],
   "source": [
    "print(f'accuracy: {model.accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.82      0.68       488\n",
      "           1       0.81      0.52      0.63      1326\n",
      "           2       0.60      0.70      0.64       659\n",
      "           3       0.25      0.64      0.36        73\n",
      "           4       0.61      0.59      0.60       549\n",
      "           5       0.49      0.79      0.60       111\n",
      "\n",
      "    accuracy                           0.63      3206\n",
      "   macro avg       0.56      0.68      0.59      3206\n",
      "weighted avg       0.67      0.63      0.63      3206\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.classification_report)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we could try different embeddings dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 399999/400000 [00:06<00:00, 58432.49it/s]\n"
     ]
    }
   ],
   "source": [
    "embeddings = np.zeros((len(texts), 50))\n",
    "glove = torchtext.vocab.GloVe(name='6B', dim=50)\n",
    "for i in range(len(texts)):\n",
    "    embeddings[i] = glove.get_vecs_by_tokens(texts[i].split(' '), lower_case_backup=True).mean(dim=0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BasicModel(embeddings, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.6575665160497383\n"
     ]
    }
   ],
   "source": [
    "print(f'accuracy: {model.accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.82      0.68       488\n",
      "           1       0.80      0.47      0.59      1326\n",
      "           2       0.56      0.65      0.60       659\n",
      "           3       0.18      0.67      0.28        73\n",
      "           4       0.57      0.50      0.53       549\n",
      "           5       0.42      0.84      0.56       111\n",
      "\n",
      "    accuracy                           0.58      3206\n",
      "   macro avg       0.52      0.66      0.54      3206\n",
      "weighted avg       0.65      0.58      0.59      3206\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 399999/400000 [00:10<00:00, 37945.69it/s]\n"
     ]
    }
   ],
   "source": [
    "embeddings = np.zeros((len(texts), 100))\n",
    "glove = torchtext.vocab.GloVe(name='6B', dim=100)\n",
    "for i in range(len(texts)):\n",
    "    embeddings[i] = glove.get_vecs_by_tokens(texts[i].split(' '), lower_case_backup=True).mean(dim=0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BasicModel(embeddings, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.677253338496227\n"
     ]
    }
   ],
   "source": [
    "print(f'accuracy: {model.accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.82      0.69       488\n",
      "           1       0.81      0.50      0.62      1326\n",
      "           2       0.59      0.68      0.63       659\n",
      "           3       0.21      0.71      0.32        73\n",
      "           4       0.60      0.53      0.56       549\n",
      "           5       0.44      0.82      0.57       111\n",
      "\n",
      "    accuracy                           0.61      3206\n",
      "   macro avg       0.54      0.68      0.57      3206\n",
      "weighted avg       0.67      0.61      0.61      3206\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.classification_report)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we could try different embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/wiki.en.vec: 6.60GB [05:27, 20.1MB/s]                                                                                                                                                   \n",
      "  0%|                                                                                                                                                                     | 0/2519370 [00:00<?, ?it/s]Skipping token b'2519370' with 1-dimensional vector [b'300']; likely a header\n",
      "100%|ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 2519370/2519370 [02:43<00:00, 15375.47it/s]\n"
     ]
    }
   ],
   "source": [
    "embeddings = np.zeros((len(texts), 300))\n",
    "fast_text = torchtext.vocab.FastText(language='en')\n",
    "for i in range(len(texts)):\n",
    "    embeddings[i] = fast_text.get_vecs_by_tokens(texts[i].split(' '), lower_case_backup=True).mean(dim=0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BasicModel(embeddings, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.6841123405974127\n"
     ]
    }
   ],
   "source": [
    "print(f'accuracy: {model.accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.83      0.69       488\n",
      "           1       0.81      0.52      0.63      1326\n",
      "           2       0.59      0.69      0.64       659\n",
      "           3       0.23      0.64      0.34        73\n",
      "           4       0.62      0.57      0.60       549\n",
      "           5       0.49      0.85      0.62       111\n",
      "\n",
      "    accuracy                           0.62      3206\n",
      "   macro avg       0.56      0.68      0.59      3206\n",
      "weighted avg       0.67      0.62      0.63      3206\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.classification_report)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Vectorization Techniques with Additional Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs = normalize(np.expand_dims(costs, 1), axis=1)\n",
    "contributions = normalize(np.expand_dims(contributions, 1), axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-Idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = hstack((corpus_tfidf_sparse, csr_matrix(costs), csr_matrix(contributions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BasicModel(dataset, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.6600972534524302\n"
     ]
    }
   ],
   "source": [
    "print(f'accuracy: {model.accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.82      0.70       488\n",
      "           1       0.79      0.60      0.68      1326\n",
      "           2       0.61      0.73      0.67       659\n",
      "           3       0.38      0.41      0.40        73\n",
      "           4       0.64      0.64      0.64       549\n",
      "           5       0.65      0.76      0.70       111\n",
      "\n",
      "    accuracy                           0.67      3206\n",
      "   macro avg       0.61      0.66      0.63      3206\n",
      "weighted avg       0.69      0.67      0.67      3206\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.classification_report)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = hstack((corpus_ldag_sparse, csr_matrix(costs), csr_matrix(contributions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BasicModel(dataset, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'accuracy: {model.accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.classification_report)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.zeros((len(texts), 300))\n",
    "glove = torchtext.vocab.GloVe(name='6B', dim=300)\n",
    "for i in range(len(texts)):\n",
    "    embeddings[i] = glove.get_vecs_by_tokens(texts[i].split(' '), lower_case_backup=True).mean(dim=0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.concatenate((embeddings, costs, contributions), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BasicModel(dataset, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.6770114603067362\n"
     ]
    }
   ],
   "source": [
    "print(f'accuracy: {model.accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.82      0.68       488\n",
      "           1       0.81      0.52      0.63      1326\n",
      "           2       0.60      0.70      0.64       659\n",
      "           3       0.25      0.64      0.36        73\n",
      "           4       0.61      0.59      0.60       549\n",
      "           5       0.49      0.79      0.60       111\n",
      "\n",
      "    accuracy                           0.63      3206\n",
      "   macro avg       0.56      0.68      0.59      3206\n",
      "weighted avg       0.67      0.63      0.63      3206\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.classification_report)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try different embeddings sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 399999/400000 [00:06<00:00, 61180.77it/s]\n"
     ]
    }
   ],
   "source": [
    "embeddings = np.zeros((len(texts), 50))\n",
    "glove = torchtext.vocab.GloVe(name='6B', dim=50)\n",
    "for i in range(len(texts)):\n",
    "    embeddings[i] = glove.get_vecs_by_tokens(texts[i].split(' '), lower_case_backup=True).mean(dim=0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.concatenate((embeddings, costs, contributions), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BasicModel(dataset, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.659179933268497\n"
     ]
    }
   ],
   "source": [
    "print(f'accuracy: {model.accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.81      0.68       488\n",
      "           1       0.79      0.47      0.59      1326\n",
      "           2       0.56      0.64      0.60       659\n",
      "           3       0.17      0.67      0.28        73\n",
      "           4       0.57      0.50      0.54       549\n",
      "           5       0.44      0.86      0.58       111\n",
      "\n",
      "    accuracy                           0.58      3206\n",
      "   macro avg       0.52      0.66      0.54      3206\n",
      "weighted avg       0.65      0.58      0.59      3206\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 399999/400000 [00:10<00:00, 39145.82it/s]\n"
     ]
    }
   ],
   "source": [
    "embeddings = np.zeros((len(texts), 100))\n",
    "glove = torchtext.vocab.GloVe(name='6B', dim=100)\n",
    "for i in range(len(texts)):\n",
    "    embeddings[i] = glove.get_vecs_by_tokens(texts[i].split(' '), lower_case_backup=True).mean(dim=0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.concatenate((embeddings, costs, contributions), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BasicModel(dataset, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.6788271121206221\n"
     ]
    }
   ],
   "source": [
    "print(f'accuracy: {model.accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.83      0.69       488\n",
      "           1       0.80      0.50      0.61      1326\n",
      "           2       0.58      0.67      0.62       659\n",
      "           3       0.21      0.73      0.32        73\n",
      "           4       0.60      0.52      0.56       549\n",
      "           5       0.44      0.83      0.57       111\n",
      "\n",
      "    accuracy                           0.60      3206\n",
      "   macro avg       0.54      0.68      0.56      3206\n",
      "weighted avg       0.66      0.60      0.61      3206\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.zeros((len(texts), 50))\n",
    "glove = torchtext.vocab.GloVe(name='6B', dim=50)\n",
    "for i in range(len(texts)):\n",
    "    embeddings[i] = glove.get_vecs_by_tokens(texts[i].split(' '), lower_case_backup=True).mean(dim=0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.concatenate((embeddings, costs, contributions), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BasicModel(dataset, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.659179933268497\n"
     ]
    }
   ],
   "source": [
    "print(f'accuracy: {model.accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.81      0.68       488\n",
      "           1       0.79      0.47      0.59      1326\n",
      "           2       0.56      0.64      0.60       659\n",
      "           3       0.17      0.67      0.28        73\n",
      "           4       0.57      0.50      0.54       549\n",
      "           5       0.44      0.86      0.58       111\n",
      "\n",
      "    accuracy                           0.58      3206\n",
      "   macro avg       0.52      0.66      0.54      3206\n",
      "weighted avg       0.65      0.58      0.59      3206\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.classification_report)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we also explore another type of embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                 | 0/2519370 [00:00<?, ?it/s]Skipping token b'2519370' with 1-dimensional vector [b'300']; likely a header\n",
      "100%|ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 2519370/2519370 [02:54<00:00, 14472.03it/s]\n"
     ]
    }
   ],
   "source": [
    "embeddings = np.zeros((len(texts), 300))\n",
    "fast_text = torchtext.vocab.FastText(language='en')\n",
    "for i in range(len(texts)):\n",
    "    embeddings[i] = fast_text.get_vecs_by_tokens(texts[i].split(' '), lower_case_backup=True).mean(dim=0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.concatenate((embeddings, costs, contributions), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BasicModel(dataset, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.6829134109807464\n"
     ]
    }
   ],
   "source": [
    "print(f'accuracy: {model.accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.83      0.69       488\n",
      "           1       0.81      0.51      0.62      1326\n",
      "           2       0.59      0.67      0.63       659\n",
      "           3       0.21      0.67      0.33        73\n",
      "           4       0.61      0.56      0.58       549\n",
      "           5       0.49      0.86      0.62       111\n",
      "\n",
      "    accuracy                           0.62      3206\n",
      "   macro avg       0.55      0.68      0.58      3206\n",
      "weighted avg       0.67      0.62      0.62      3206\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.classification_report)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding(texts,num_topics):\n",
    "    # Create a corpus\n",
    "    D = Dictionary([s.split(' ') for s in texts])\n",
    "    n_tokens = len(D)\n",
    "    reviews_bow = [D.doc2bow(doc.split(' ')) for doc in texts]\n",
    "    # compute using a LDA algorithm for topic modelling\n",
    "    ldag = LdaModel(corpus=reviews_bow, id2word=D, num_topics=num_topics)\n",
    "    return ldag, reviews_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldag, reviews_bow = embedding(texts,500) # Numer of topics = 500"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Corpus processing and analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_size = len(reviews_bow)\n",
    "print(f\"Dataset contains {corpus_size} documents\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** We note that it has 32,052 documents. Since this is a very high number, we will limit the number of documents to be analyzed to about 1000 documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_docs = 1000\n",
    "random.seed(10)\n",
    "corpus = random.sample(reviews_bow, num_docs)\n",
    "print(f\"Dataset contains {len(corpus)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_ldag = ldag[corpus]\n",
    "X = corpus2csc(corpus_ldag).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = X.shape[1]\n",
    "print(f\"Number of topics: {n_topics}\")\n",
    "print(f\"X: sparse matrix with {X.nnz} nonzero values out of {num_docs * n_topics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that the embedding is shown in sparse format, showing the non-zero values only. The proportion of zero entries is\n",
    "print(f\"{(1 - X.nnz / num_docs / n_topics) * 100:.2f} % \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All other entries are values in  [0,1] . \n",
    "# Since the embedding space is probabilistic the row vectors should sum up to one. \n",
    "# However, we can check that this is no exactly the case: \n",
    "average_row = np.mean(X.sum(axis=1).T)\n",
    "print(f\"Average row sum: {average_row:.2f}\")\n",
    "if average_row != 1.0: # If the row of vectros are not sum to one, we need to normalize this.\n",
    "    X = scsp.csr_matrix(X / np.sum(X, axis=1))\n",
    "average_row = np.mean(X.sum(axis=1).T)\n",
    "print(f\"Average row sum after normalization: {average_row:.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Similarity Measures\n",
    "\n",
    "In order to construct a semantic graph from a collection of document embeddings, a similary measure between two embeddings,  ð±  and  ð² , is required.\n",
    "\n",
    "To do this, we define differents steps:\n",
    "\n",
    "1. Determine the most and the least similar documents in the dataframe, according to the BC similarity measure.\n",
    "2. Compute the indices of the least similar and the most similar documents in the dataframe (excluding the document itself).\n",
    "3. Compute a similarity matrix  ð for a similarity graph."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Similarity between documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FunciÃ³n que nos devuelve la similitud de los documentos con el documento i\n",
    "def similarity_docs(i,X): \n",
    "    xi = X[i, :]\n",
    "    return np.multiply(np.sqrt(xi),np.sqrt(X[1:].T)) # Excluimos el elemento i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similitud con el documento 0\n",
    "print(similarity_docs(0,X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BC = similarity_docs(0,X)\n",
    "arg_max = np.argmax(BC)\n",
    "print('Most similar: ',arg_max)\n",
    "arg_min = np.argmin(BC)\n",
    "print('Least similar: ',arg_min)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Compute the similarity matrix S based on the BC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = np.multiply(np.sqrt(X),np.sqrt(X.T))\n",
    "\n",
    "print('Number of nonzero:',S.nnz)\n",
    "print(f\"NON-ZERO proportion = {(S.nnz/(S.shape[0]*S.shape[1]))*100:0.2f} %\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the matrix is symmetric so we keep only with upper triangular part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = triu(S,k = 1) # above diagonal\n",
    "print('Number of non-zero components in S:', S.nnz )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Graph\n",
    "\n",
    "Para la representaciÃ³n de los grafos seguiremos, hemos optado por seguir los siguientes pasos:\n",
    "\n",
    "1. Graph construction\n",
    "2. Visualize Graphs with layout algorithms\n",
    "3. Visualize Graphs with Community detection algorithms\n",
    "4. Impact"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Graph Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_nodes = num_docs                                  # the documents in the corpus\n",
    "n_edges = S.nnz                                     # links connecting pairs of documents with nonzero similarity \n",
    "n_edges_per_node = np.round(n_edges/n_nodes)        # the similarity values \n",
    "\n",
    "print(f\"Number of nodes: {n_nodes}\")\n",
    "print(f\"Number of edges: {n_edges}\")\n",
    "print(f\"Number of edges per node: {n_edges_per_node}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 2))\n",
    "plt.hist(S.data, bins=100)\n",
    "plt.xlabel('Similarity values')\n",
    "plt.ylabel('No. of matrix entries')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 Apply a threshold to sparse matrix in order to get a subgraph with an average of 10 edges per node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set average number of edges per node\n",
    "n_edges_per_node = 10\n",
    "sample_factor = num_docs / corpus_size\n",
    "\n",
    "# Compute threshold to get the target number of edges\n",
    "\n",
    "n_edges = n_nodes * n_edges_per_node\n",
    "sort = np.sort(S.data)\n",
    "inv_sort = sort[::-1]\n",
    "thr = inv_sort[n_edges]\n",
    "\n",
    "# Apply the threshold to similarity matrix\n",
    "S.data[S.data<thr] = 0\n",
    "S.eliminate_zeros()\n",
    "\n",
    "n_links = len(S.data)/2/sample_factor**2\n",
    "\n",
    "print(f\"Threshold: {thr:.4f}\")\n",
    "print(f\"Number of edges: {n_edges}\")\n",
    "print(f\"Estimated number of links in full corpus: {n_links:.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Layout Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.from_scipy_sparse_matrix(S)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 Testing Random Layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##########################\n",
    "# Testing a random  layout #\n",
    "# ##########################\n",
    "\n",
    "positions = nx.drawing.layout.random_layout(G)\n",
    "plt.figure(figsize=(5,5))\n",
    "nx.draw(G, positions, node_size=10, width=0.10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 Testing a Fruchterman-Reingold: Forced-directed algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "#       Testing a Fruchterman-Reingold: Forced-directed algorithm      #\n",
    "########################################################################\n",
    "# Compute positions\n",
    "positions = nx.spring_layout(G, iterations = 50, seed = 0)\n",
    "\n",
    "# Draw graph\n",
    "nx.draw(G, positions, node_size=1, width=0.02)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3 Largest connected component (LCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "#           Largest connected component (LCC) from the graph           #\n",
    "########################################################################\n",
    "\n",
    "largest_cc = max(nx.connected_components(G), key=len)\n",
    "G_lcc = G.subgraph(largest_cc) # saving LCC in a subgraph\n",
    "nx.draw(G_lcc, positions, node_size=1, width=0.02)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Community detection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1 Define community algorith to detect communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx.algorithms.community as nx_comm\n",
    "\n",
    "# Detecting communities in a graph using the greedy modularity algorithm. \n",
    "# This function takes a graph and a resolution parameter as arguments and returns a list of communities detected in the graph.\n",
    "# The resolution parameter of the community.greedy_modularity_communities() function controls \n",
    "# the amount of clustering that is performed on the network. \n",
    "# To determine the optim value of resolution we use differents numbers to study the modularity.\n",
    "# (Modularity is a measure used to evaluate the quality of a partition of a network into communities or groups of nodes.)\n",
    "for resolution in [0.5, 1, 1.5, 2, 2.5 , 3, 3.5]:\n",
    "    C = nx_comm.greedy_modularity_communities(G_lcc, resolution=resolution)\n",
    "    # Modularity of the partition\n",
    "    modularity = nx_comm.modularity(G_lcc, C)\n",
    "    nc = len(C)\n",
    "    print(f\"Number of communities: {nc}\")\n",
    "    print(f\"Modularity: {modularity:.2f}\")\n",
    "    print(f\"Resolution: {resolution}\")\n",
    "    print(\"\\n---------------------------\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best value for the resolution is 2 since the modularity value is the highest. This means that the quality of the partition is higher than the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = nx_comm.greedy_modularity_communities(G_lcc, resolution=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 Check that the size of the communities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort communities by decreasing size\n",
    "C = sorted(C, key=len, reverse=True)\n",
    "# Number of communities\n",
    "nc = len(C)\n",
    "\n",
    "# Size of each community\n",
    "comm_sizes = [len(x) for x in C]\n",
    "\n",
    "plt.figure(figsize=(8, 2))\n",
    "plt.stem(range(nc), comm_sizes)\n",
    "plt.xlabel('Community')\n",
    "plt.ylabel('Size')\n",
    "plt.grid('on')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3 Draw graph with color palette to show the differents communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = sns.color_palette(palette=\"Paired\", n_colors=nc)\n",
    "node2comm = {n: 0 for n in G_lcc}\n",
    "for i, com in enumerate(C):\n",
    "    for node in list(com):\n",
    "        node2comm[node] = i\n",
    "\n",
    "# Map node attribute to rgb colors\n",
    "node_colors = [palette[node2comm[node]] for node in G_lcc]\n",
    "\n",
    "# Get list of degrees\n",
    "degrees = [degree[1] for degree in G_lcc.degree]\n",
    "\n",
    "#  Draw graph\n",
    "nx.draw(G_lcc, positions, node_size=degrees, width=0.02, \n",
    "        node_color=node_colors)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.4 Quality Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage, performance = nx_comm.partition_quality(G_lcc, C)\n",
    "\n",
    "print(f\"Coverage: {coverage}\")\n",
    "print(f\"Performance: {performance}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Results obtained:*\n",
    "\n",
    "**COVERAGE**:  is a metric used to evaluate the ability of a natural language model to process a dataset. It is defined as the proportion of words in the dataset that are recognized by the model. A higher coverage value indicates a greater ability of the model to process the dataset.\n",
    "\n",
    "**PERFORMANCE**: is a metric used to evaluate the accuracy of a natural language model. It can be measured in different ways, depending on the type of task being performed\n",
    "\n",
    "Since both the performance and coverage values are high, the results obtained are good.\n",
    "As a noteworthy aspect of the values obtained is that the coverage is lower than the performance. This means that the natural language model is able to process a smaller amount of words in the dataset, but it is more accurate in performing the assigned task."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.5 Community networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert matrix to csr\n",
    "S2 = scsp.csr_matrix(S + S.T)\n",
    "\n",
    "# Initialize matrix for the community graph\n",
    "M = np.zeros((nc, nc))\n",
    "\n",
    "# Loop over the communities\n",
    "for i in range(nc):\n",
    "    nodes_i = list(C[i])\n",
    "    ni = len(nodes_i)\n",
    "    \n",
    "    # Loop over the communities\n",
    "    for j in range(i+1, nc):\n",
    "        nodes_j = list(C[j])\n",
    "        n_j = len(nodes_j)\n",
    "\n",
    "        # Select submatrix Sij, with rows from Ci and columns from Cj\n",
    "        Sij = S2[nodes_i][:, nodes_j] \n",
    "        \n",
    "        # Compute M\n",
    "        M[i, j] = (1/(ni*n_j))*np.sum(np.sum(Sij))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CG = nx.from_numpy_matrix(M)\n",
    "positions_CG = nx.spring_layout(CG, iterations = 50, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute edge widths  & node sizes\n",
    "w = [e[2]['weight']*50 for e in CG.edges(data=True)]\n",
    "degrees = [val * 80 for (node, val) in CG.degree()]\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "nx.draw(CG, positions_CG, node_size=degrees, width=w, node_color=palette, with_labels=True)\n",
    "plt.title(\"Community structure\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Impact\n",
    "\n",
    "In this section we will explore measures to evaluate the role of a node in its network, using a family of centrality measures, that evaluate nodes according to their position in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to  print top 3 nodes from the graphs\n",
    "def print_top3_nodes(impact):\n",
    "    sorted_ctr = sorted(impact, key=lambda x: x[1], reverse=True)\n",
    "    ctr3 = sorted_ctr[2][1]\n",
    "    print(ctr3)\n",
    "\n",
    "    eps = 1e-5\n",
    "    node_sizes = [20 * x[1] / ctr3 * (x[1] > ctr3 - eps) for x in impact]\n",
    "\n",
    "    # Plot graph\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    nx.draw(G_lcc, positions, node_size=node_sizes, node_color=node_colors, width=0.1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1 Degree centrality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute centrality\n",
    "centrality = [(k, v) for k, v in nx.degree_centrality(G_lcc).items()]\n",
    "\n",
    "# Print the sum of all centrality values:\n",
    "sum_gw = np.sum(list(nx.degree_centrality(G_lcc).values()))\n",
    "print(f\"Sum of all graph weights: {sum_gw:.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the top 3 nodes with the highest centrality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_top3_nodes(centrality)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2 Closeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_edges = nx.get_edge_attributes(G_lcc, 'weight')\n",
    "\n",
    "edges = {}\n",
    "for k, v in dict_edges.items(): edges[k] = v\n",
    "\n",
    "nx.set_edge_attributes(G_lcc, edges, 'distance')\n",
    "\n",
    "# Compute centrality\n",
    "centrality = nx.closeness_centrality(G_lcc, distance = 'distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_top3_nodes(centrality.items())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.3 Betweenness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute centrality\n",
    "centrality = nx.betweenness_centrality(G_lcc)\n",
    "print_top3_nodes(centrality.items())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.4 PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centrality = nx.pagerank(G_lcc)\n",
    "node_sizes = [4000 * x * (x > 0.00185) for x in centrality.values()]\n",
    "plt.figure(figsize=(4,4))\n",
    "nx.draw(G_lcc, positions, node_size=node_sizes, node_color=node_colors, width=0.1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Second Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, labels, costs, contributions = extract_data('data/projects.xlsx', 'data/SciVocCodes.xlsx', 1)\n",
    "texts = preprocessing(texts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-Idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get texts in right format\n",
    "D = Dictionary([sentence.split(' ') for sentence in texts])\n",
    "n_tokens = len(D)\n",
    "reviews_bow = [D.doc2bow(doc.split(' ')) for doc in texts]\n",
    "\n",
    "# compute tfidf representations\n",
    "tfidf = TfidfModel(reviews_bow)\n",
    "reviews_tfidf = tfidf[reviews_bow]\n",
    "\n",
    "# create sparse corpus\n",
    "num_docs = len(reviews_bow)\n",
    "corpus_tfidf_sparse = corpus2csc(reviews_tfidf, num_terms=n_tokens, num_docs=num_docs).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BasicModel(corpus_tfidf_sparse, labels, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.37647629711564673\n"
     ]
    }
   ],
   "source": [
    "print(f'accuracy: {model.accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.39      0.33       188\n",
      "           1       0.54      0.53      0.54       582\n",
      "          10       0.48      0.45      0.46       497\n",
      "          11       0.15      0.14      0.15        35\n",
      "          12       0.40      0.40      0.40        68\n",
      "          13       0.30      0.17      0.21        36\n",
      "          14       0.38      0.43      0.40       282\n",
      "          15       0.27      0.08      0.12        53\n",
      "          16       0.52      0.59      0.55       636\n",
      "          17       0.40      0.42      0.41        80\n",
      "          18       0.00      0.00      0.00        12\n",
      "          19       0.52      0.49      0.51       554\n",
      "           2       0.45      0.58      0.51       201\n",
      "          20       0.34      0.51      0.40       176\n",
      "          21       0.32      0.36      0.34        91\n",
      "          22       0.00      0.00      0.00        13\n",
      "          23       0.31      0.52      0.39       139\n",
      "          24       0.29      0.28      0.28        40\n",
      "          26       1.00      0.12      0.22         8\n",
      "          27       0.42      0.37      0.40        51\n",
      "          28       0.58      0.46      0.51       580\n",
      "          29       0.36      0.29      0.32        93\n",
      "           3       0.51      0.64      0.57       564\n",
      "          31       0.41      0.53      0.46       266\n",
      "          32       0.61      0.57      0.59       315\n",
      "          33       0.50      0.34      0.41       424\n",
      "          34       0.00      0.00      0.00        14\n",
      "          35       0.27      0.34      0.30        41\n",
      "          36       0.37      0.55      0.44        84\n",
      "          37       0.54      0.40      0.46        62\n",
      "          38       0.63      0.58      0.60        67\n",
      "          39       0.30      0.56      0.39        96\n",
      "           4       0.00      0.00      0.00         1\n",
      "          40       0.00      0.00      0.00        20\n",
      "           5       0.61      0.55      0.58       726\n",
      "           6       0.43      0.54      0.48       287\n",
      "           7       0.49      0.46      0.48       883\n",
      "           8       0.63      0.48      0.55      1029\n",
      "           9       0.60      0.53      0.56       292\n",
      "\n",
      "    accuracy                           0.49      9586\n",
      "   macro avg       0.39      0.38      0.37      9586\n",
      "weighted avg       0.50      0.49      0.49      9586\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.classification_report)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_ldag_sparse = lda_model(reviews_bow, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BasicModel(corpus_ldag_sparse, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'accuracy: {model.accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.classification_report)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.zeros((len(texts), 300))\n",
    "glove = torchtext.vocab.GloVe(name='6B', dim=300)\n",
    "for i in range(len(texts)):\n",
    "    embeddings[i] = glove.get_vecs_by_tokens(texts[i].split(' '), lower_case_backup=True).mean(dim=0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BasicModel(embeddings, labels, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.36807103364739163\n"
     ]
    }
   ],
   "source": [
    "print(f'accuracy: {model.accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.20      0.35      0.25       188\n",
      "           1       0.50      0.49      0.50       582\n",
      "          10       0.48      0.41      0.44       497\n",
      "          11       0.11      0.29      0.16        35\n",
      "          12       0.21      0.46      0.29        68\n",
      "          13       0.14      0.22      0.17        36\n",
      "          14       0.32      0.39      0.35       282\n",
      "          15       0.12      0.17      0.14        53\n",
      "          16       0.53      0.50      0.52       636\n",
      "          17       0.22      0.46      0.29        80\n",
      "          18       0.17      0.08      0.11        12\n",
      "          19       0.48      0.40      0.44       554\n",
      "           2       0.38      0.64      0.48       201\n",
      "          20       0.26      0.48      0.34       176\n",
      "          21       0.19      0.42      0.26        91\n",
      "          22       0.00      0.00      0.00        13\n",
      "          23       0.24      0.35      0.28       139\n",
      "          24       0.19      0.25      0.22        40\n",
      "          26       0.25      0.12      0.17         8\n",
      "          27       0.37      0.49      0.42        51\n",
      "          28       0.56      0.38      0.46       580\n",
      "          29       0.17      0.37      0.23        93\n",
      "           3       0.51      0.56      0.54       564\n",
      "          31       0.39      0.47      0.43       266\n",
      "          32       0.50      0.50      0.50       315\n",
      "          33       0.48      0.28      0.35       424\n",
      "          34       0.12      0.14      0.13        14\n",
      "          35       0.17      0.27      0.21        41\n",
      "          36       0.33      0.52      0.41        84\n",
      "          37       0.32      0.42      0.36        62\n",
      "          38       0.55      0.67      0.60        67\n",
      "          39       0.29      0.50      0.37        96\n",
      "           4       0.00      0.00      0.00         1\n",
      "          40       0.00      0.00      0.00        20\n",
      "           5       0.62      0.47      0.54       726\n",
      "           6       0.42      0.53      0.47       287\n",
      "           7       0.51      0.34      0.41       883\n",
      "           8       0.66      0.42      0.51      1029\n",
      "           9       0.51      0.53      0.52       292\n",
      "\n",
      "    accuracy                           0.43      9586\n",
      "   macro avg       0.32      0.37      0.33      9586\n",
      "weighted avg       0.48      0.43      0.44      9586\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
